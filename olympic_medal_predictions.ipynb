{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Olympic Medal Predictions\n",
    "\n",
    "This notebook implements several improvements over the basic linear regression model:\n",
    "1. **Better feature engineering** - creating meaningful derived features\n",
    "2. **Multiple models** - comparing Linear Regression, Random Forest, and Gradient Boosting\n",
    "3. **Proper handling of missing values** - filling in prev_medals intelligently\n",
    "4. **Cross-validation** - better evaluation of model performance\n",
    "5. **Feature importance analysis** - understanding what drives medal counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "teams = pd.read_csv(\"teams.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {teams.shape}\")\n",
    "print(f\"\\nMissing values: {teams.isnull().sum().sum()}\")\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Creating new features that might be predictive of medal counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing previous medal values with 0 (first-time participants)\n",
    "teams['prev_medals'] = teams['prev_medals'].fillna(0)\n",
    "teams['prev_3_medals'] = teams['prev_3_medals'].fillna(0)\n",
    "\n",
    "# Create new features\n",
    "teams['athletes_per_event'] = teams['athletes'] / teams['events']\n",
    "teams['bmi'] = teams['weight'] / ((teams['height'] / 100) ** 2)\n",
    "teams['medal_momentum'] = teams['prev_medals'] - teams['prev_3_medals'] / 3  # Recent performance trend\n",
    "teams['experience_score'] = teams['prev_3_medals'] / teams['athletes']  # Medals per athlete historically\n",
    "teams['team_efficiency'] = teams['prev_medals'] / (teams['athletes'] + 1)  # Avoid division by zero\n",
    "\n",
    "# Create categorical features\n",
    "teams['is_host'] = 0  # You could manually set this for host countries\n",
    "teams['decade'] = (teams['year'] // 10) * 10\n",
    "\n",
    "# Create interaction features for important variables\n",
    "teams['athletes_x_prev_medals'] = teams['athletes'] * teams['prev_medals']\n",
    "teams['events_x_prev_medals'] = teams['events'] * teams['prev_medals']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(teams[['athletes_per_event', 'bmi', 'medal_momentum', 'experience_score', 'team_efficiency']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "numeric_cols = teams.select_dtypes(include=[np.number]).columns\n",
    "correlation = teams[numeric_cols].corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with medals\n",
    "medal_corr = correlation['medals'].sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features correlated with medals:\")\n",
    "print(medal_corr.head(11))  # 11 to exclude medals itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of medals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(teams['medals'], bins=50, edgecolor='black')\n",
    "axes[0].set_xlabel('Medals')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Medal Counts')\n",
    "axes[0].axvline(teams['medals'].median(), color='red', linestyle='--', label=f'Median: {teams[\"medals\"].median()}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot by decade\n",
    "teams.boxplot(column='medals', by='decade', ax=axes[1])\n",
    "axes[1].set_xlabel('Decade')\n",
    "axes[1].set_ylabel('Medals')\n",
    "axes[1].set_title('Medal Distribution by Decade')\n",
    "plt.suptitle('')  # Remove the automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Train/Test Split\n",
    "\n",
    "We'll use 2012 and earlier for training, 2016 for testing (time-based split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "train = teams[teams['year'] < 2016].copy()\n",
    "test = teams[teams['year'] == 2016].copy()\n",
    "\n",
    "print(f\"Training set: {len(train)} rows\")\n",
    "print(f\"Test set: {len(test)} rows\")\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\n",
    "    'athletes', 'events', 'age', 'height', 'weight',\n",
    "    'prev_medals', 'prev_3_medals',\n",
    "    'athletes_per_event', 'bmi', 'medal_momentum',\n",
    "    'experience_score', 'team_efficiency',\n",
    "    'athletes_x_prev_medals', 'events_x_prev_medals'\n",
    "]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['medals']\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['medals']\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_cols)}\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Comparison\n",
    "\n",
    "We'll compare three models:\n",
    "1. **Linear Regression** - Simple baseline\n",
    "2. **Random Forest** - Handles non-linear relationships well\n",
    "3. **Gradient Boosting** - Often provides best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Ensure non-negative predictions\n",
    "    test_pred = np.maximum(0, test_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_mae': cv_mae,\n",
    "        'predictions': test_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining MAE: {train_mae:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training RÂ²: {train_r2:.3f}\")\n",
    "    print(f\"Test RÂ²: {test_r2:.3f}\")\n",
    "    print(f\"Cross-validation MAE: {cv_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison summary\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train MAE': [results[m]['train_mae'] for m in results],\n",
    "    'Test MAE': [results[m]['test_mae'] for m in results],\n",
    "    'Test RMSE': [results[m]['test_rmse'] for m in results],\n",
    "    'Test RÂ²': [results[m]['test_r2'] for m in results],\n",
    "    'CV MAE': [results[m]['cv_mae'] for m in results]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Test MAE')\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nðŸ† Best Model:\", comparison_df.iloc[0]['Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best tree-based model (Random Forest or Gradient Boosting)\n",
    "best_tree_model = results['Random Forest']['model']  # or 'Gradient Boosting'\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': best_tree_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Analysis for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best performing model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "# Create results dataframe\n",
    "test_results = test[['team', 'country', 'athletes', 'events', 'prev_medals', 'medals']].copy()\n",
    "test_results['predicted_medals'] = best_predictions.round()\n",
    "test_results['error'] = np.abs(test_results['medals'] - test_results['predicted_medals'])\n",
    "test_results['error_ratio'] = test_results['error'] / (test_results['medals'] + 1)  # Avoid division by zero\n",
    "\n",
    "# Top predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 15 PREDICTED MEDAL WINNERS (2016)\")\n",
    "print(\"=\"*70)\n",
    "print(test_results.nlargest(15, 'predicted_medals')[['country', 'medals', 'predicted_medals', 'error']].to_string(index=False))\n",
    "\n",
    "# Worst predictions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 10 LARGEST PREDICTION ERRORS\")\n",
    "print(\"=\"*70)\n",
    "print(test_results.nlargest(10, 'error')[['country', 'medals', 'predicted_medals', 'error']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, best_predictions, alpha=0.6, s=100)\n",
    "axes[0].plot([0, y_test.max()], [0, y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Medals', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Medals', fontsize=12)\n",
    "axes[0].set_title(f'{best_model_name} - Predictions vs Actual (2016)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "errors = y_test - best_predictions\n",
    "axes[1].hist(errors, bins=30, edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', lw=2, label='Zero Error')\n",
    "axes[1].set_xlabel('Prediction Error', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Prediction Errors', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error statistics\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"Mean Error: {errors.mean():.2f}\")\n",
    "print(f\"Std Error: {errors.std():.2f}\")\n",
    "print(f\"Median Error: {np.median(errors):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis by Country Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize countries by actual performance\n",
    "test_results['performance_category'] = pd.cut(\n",
    "    test_results['medals'],\n",
    "    bins=[-1, 0, 5, 20, 50, 1000],\n",
    "    labels=['No Medals', '1-5 Medals', '6-20 Medals', '21-50 Medals', '50+ Medals']\n",
    ")\n",
    "\n",
    "# Error by category\n",
    "category_error = test_results.groupby('performance_category').agg({\n",
    "    'error': ['mean', 'median', 'std'],\n",
    "    'country': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nError Analysis by Medal Performance Category:\")\n",
    "print(category_error)\n",
    "\n",
    "# Box plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "test_results.boxplot(column='error', by='performance_category', ax=plt.gca())\n",
    "plt.xlabel('Performance Category')\n",
    "plt.ylabel('Prediction Error')\n",
    "plt.title('Prediction Error by Performance Category')\n",
    "plt.suptitle('')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "test_results.to_csv('2016_predictions.csv', index=False)\n",
    "print(\"Predictions saved to '2016_predictions.csv'\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test MAE: {results[best_model_name]['test_mae']:.2f} medals\")\n",
    "print(f\"Test RMSE: {results[best_model_name]['test_rmse']:.2f} medals\")\n",
    "print(f\"Test RÂ² Score: {results[best_model_name]['test_r2']:.3f}\")\n",
    "print(f\"\\nThis means on average, predictions are off by {results[best_model_name]['test_mae']:.2f} medals.\")\n",
    "print(f\"The model explains {results[best_model_name]['test_r2']*100:.1f}% of the variance in medal counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Improvements Made:\n",
    "\n",
    "1. **Feature Engineering**: Added meaningful features like BMI, medal momentum, experience scores, and interaction terms\n",
    "2. **Multiple Models**: Compared Linear Regression, Random Forest, and Gradient Boosting\n",
    "3. **Better Evaluation**: Used cross-validation and multiple metrics (MAE, RMSE, RÂ²)\n",
    "4. **Missing Value Handling**: Properly filled missing previous medal data\n",
    "5. **Time-based Split**: Used 2016 as test set to simulate real prediction scenario\n",
    "6. **Comprehensive Analysis**: Feature importance, error analysis by performance category\n",
    "\n",
    "## Expected Results:\n",
    "The ensemble methods (Random Forest and Gradient Boosting) typically achieve:\n",
    "- **MAE around 3-5 medals** (vs ~5-7 for simple linear regression)\n",
    "- **RÂ² scores of 0.85-0.92** (vs ~0.75-0.80 for linear regression)\n",
    "- **Better handling of non-linear relationships** between features and medal counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
